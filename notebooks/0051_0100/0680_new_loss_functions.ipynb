{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gnn_tracking.metrics.losses import CondensationLossTiger\n",
    "from gnn_tracking.training.callbacks import PrintValidationMetrics\n",
    "from gnn_tracking.training.tc import TCModule\n",
    "from gnn_tracking.models.graph_construction import MLGraphConstructionFromChkpt\n",
    "from gnn_tracking.models.track_condensation_networks import PreTrainedECGraphTCN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'hc_in' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['hc_in'])`.\n",
      "  rank_zero_warn(\n",
      "\u001b[36m[14:42:25] DEBUG: Getting class MLModule from module gnn_tracking.training.ml\u001b[0m\n",
      "\u001b[36m[14:42:26] DEBUG: Loading checkpoint /home/kl5675/Documents/23/git_sync/hyperparameter_optimization2/scripts/full_detector/lightning_logs/merciful-reindeer-of-coffee/checkpoints/epoch=79-step=72000.compat.ckpt\u001b[0m\n",
      "\u001b[36m[14:42:26] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001b[0m\n",
      "\u001b[36m[14:42:26] DEBUG: Getting class GraphConstructionHingeEmbeddingLoss from module gnn_tracking.metrics.losses\u001b[0m\n",
      "\u001b[36m[14:42:26] DEBUG: Getting class GraphConstructionKNNScanner from module gnn_tracking.graph_construction.k_scanner\u001b[0m\n",
      "\u001b[36m[14:42:26] DEBUG: Checkpoint loaded. Model ready to go.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = PreTrainedECGraphTCN(\n",
    "      ec=None,\n",
    "      node_indim= 38,\n",
    "      edge_indim= 76,\n",
    "      h_dim= 192,\n",
    "      e_dim= 192,\n",
    "      hidden_dim= 192,\n",
    "      h_outdim= 24,\n",
    "      L_hc= 5,\n",
    "      alpha_latent= 0.5,\n",
    "      n_embedding_coords= 24,\n",
    ")\n",
    "preproc = MLGraphConstructionFromChkpt(\n",
    "    ml_chkpt_path=\"/home/kl5675/Documents/23/git_sync/hyperparameter_optimization2/scripts/full_detector/lightning_logs/merciful-reindeer-of-coffee/checkpoints/epoch=79-step=72000.compat.ckpt\",\n",
    "    max_num_neighbors= 15,\n",
    "    max_radius= 1.,\n",
    "    use_embedding_features= True,\n",
    "    build_edge_features= True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "condensation_loss = CondensationLossTiger(\n",
    "      q_min= 0.01,\n",
    "      max_n_rep=100_000,\n",
    "      sample_hits=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = TCModule(\n",
    "    model=model,\n",
    "    preproc=preproc,\n",
    "    lw_repulsive=0.74,\n",
    "    loss_fct=condensation_loss,\n",
    "    cluster_scanner=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.loading import TrackingDataModule\n",
    "\n",
    "dm = TrackingDataModule(\n",
    "    identifier=\"point_clouds_v10\",\n",
    "    train=dict(\n",
    "        dirs=[\n",
    "            \"/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_1/\"\n",
    "        ],\n",
    "        sample_size=1000,\n",
    "        # If you run into memory issues, reduce this\n",
    "    ),\n",
    "    val=dict(\n",
    "        dirs=[\n",
    "            \"/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_9/\"\n",
    "        ],\n",
    "        stop=5\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3 ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    callbacks=[PrintValidationMetrics()],\n",
    "    # fast_dev_run=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[32m[14:42:30] INFO: DataLoader will load 900 graphs (out of 900 available).\u001b[0m\n",
      "\u001b[36m[14:42:30] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_1/data21000_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_1/data21999_s0.pt\u001b[0m\n",
      "\u001b[32m[14:42:30] INFO: DataLoader will load 5 graphs (out of 1000 available).\u001b[0m\n",
      "\u001b[36m[14:42:30] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_9/data29000_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v6/part_9/data29004_s0.pt\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                  | Params\n",
      "---------------------------------------------------\n",
      "0 | model    | PreTrainedECGraphTCN  | 1.9 M \n",
      "1 | preproc  | MLGraphConstruction   | 271 K \n",
      "2 | loss_fct | CondensationLossTiger | 0     \n",
      "---------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "271 K     Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.741     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816e15fec15f4945bd889b4a05de433a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2a8aed85044da09d054278792e41f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "Process ForkProcess-4:\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "Process ForkProcess-3:\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "  File \"/scratch/gpfs/kl5675/micromamba/envs/gnn/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8840/api/kernels/bfdf9333-20be-429b-86b7-ed72d7fd02b5/restart?1699037160704 failed, reason: socket hang up. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(oc, dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
