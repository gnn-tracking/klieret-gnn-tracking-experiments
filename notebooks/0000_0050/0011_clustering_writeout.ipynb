{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0531c62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import uniform\n",
    "from torch_geometric.loader import DataLoader\n",
    "import colorlog\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c650",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_evts, n_sectors = 10, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde54fd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gnn_tracking.preprocessing.point_cloud_builder import PointCloudBuilder\n",
    "\n",
    "# build point clouds for each sector in the pixel layers only\n",
    "pc_builder = PointCloudBuilder(\n",
    "    indir=\"/tigress/jdezoort/codalab/train_1\",\n",
    "    outdir=Path(\"~/data/gnn_tracking/point_clouds/\").expanduser(),\n",
    "    n_sectors=n_sectors,\n",
    "    pixel_only=True,\n",
    "    redo=False,\n",
    "    measurement_mode=False,\n",
    "    sector_di=0,\n",
    "    sector_ds=1.3,\n",
    "    thld=0.9,\n",
    "    log_level=1,\n",
    ")\n",
    "pc_builder.process(n=n_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020ca0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# each point cloud is a PyG Data object\n",
    "point_cloud = pc_builder.data_list\n",
    "pc_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91897cef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can build graphs on the point clouds using geometric cuts\n",
    "from gnn_tracking.graph_construction.graph_builder import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    indir=Path(\"~/data/gnn_tracking/point_clouds/\").expanduser(),\n",
    "    outdir=Path(\"~/data/gnn_tracking/graphs/\").expanduser(),\n",
    "    redo=False,\n",
    "    measurement_mode=False,\n",
    "    phi_slope_max=0.0035,\n",
    "    z0_max=200,\n",
    "    dR_max=2.3,\n",
    ")\n",
    "graph_builder.process(n=n_evts * n_sectors)\n",
    "graph_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c987bd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gnn_tracking.models.track_condensation_networks import GraphTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Utilizing {device}\")\n",
    "\n",
    "# use reference graph to get relevant dimensions\n",
    "g = graph_builder.data_list[0]\n",
    "node_indim = g.x.shape[1]\n",
    "edge_indim = g.edge_attr.shape[1]\n",
    "hc_outdim = 2  # output dim of latent space\n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "graphs = graph_builder.data_list\n",
    "n_graphs = len(graphs)\n",
    "rand_array = uniform(low=0, high=1, size=n_graphs)\n",
    "train_graphs = [g for i, g in enumerate(graphs) if (rand_array <= 0.6)[i]]\n",
    "test_graphs = [\n",
    "    g for i, g in enumerate(graphs) if ((rand_array > 0.6) & (rand_array <= 0.8))[i]\n",
    "]\n",
    "val_graphs = [g for i, g in enumerate(graphs) if (rand_array > 0.8)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 2}\n",
    "train_loader = DataLoader(list(train_graphs), **params)\n",
    "params = {\"batch_size\": 1, \"shuffle\": False, \"num_workers\": 2}\n",
    "test_loader = DataLoader(list(test_graphs), **params)\n",
    "val_loader = DataLoader(list(val_graphs), **params)\n",
    "loaders = {\"train\": train_loader, \"test\": test_loader, \"val\": val_loader}\n",
    "print(\"Loader sizes:\", [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 0.1\n",
    "loss_functions = {\n",
    "    \"edge\": EdgeWeightBCELoss().to(device),\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    # \"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"edge\": 5,\n",
    "    \"potential_attractive\": 10,\n",
    "    \"potential_repulsive\": 1,\n",
    "    \"background\": 5,\n",
    "    # \"object\": 1/250000,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = GraphTCN(node_indim, edge_indim, hc_outdim, hidden_dim=64)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"number trainable params:\", n_params)\n",
    "trainer = TCNTrainer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    loss_functions=loss_functions,\n",
    "    lr=0.0001,\n",
    "    loss_weights=loss_weights,\n",
    "    device=device,\n",
    ")\n",
    "print(trainer.loss_functions)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e601e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "out = model(g.to(device))\n",
    "h = out[\"H\"].detach().cpu().numpy()\n",
    "beta = out[\"B\"].detach().cpu().numpy()\n",
    "w = out[\"W\"]\n",
    "particle_id = g.particle_id.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec8141",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for ievt in range(10):\n",
    "    for isec in range(32):\n",
    "        i = ievt * 32 + isec\n",
    "        g = graph_builder.data_list[i]\n",
    "        particle_id = g.particle_id.cpu()\n",
    "        out = model(g.to(device))\n",
    "        h = out[\"H\"].detach().cpu().numpy()\n",
    "        with open(f\"{ievt}_{isec}.npy\", \"wb\") as outf:\n",
    "            np.save(outf, h)\n",
    "        with open(f\"t_{ievt}_{isec}.npy\", \"wb\") as outf:\n",
    "            np.save(outf, particle_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40c6a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa603cde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DBScanner:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.opt_clusters = None\n",
    "\n",
    "        self.metrics = collections.defaultdict(list)\n",
    "        self.eps = []\n",
    "        self.min_pts = []\n",
    "\n",
    "    def scan(self, h, particle_id, n_trials=100):\n",
    "        for _ in range(n_trials):\n",
    "            eps = np.random.uniform()\n",
    "            min_pts = np.random.randint(1, 5)\n",
    "            self.eps.append(eps)\n",
    "            self.min_pts.append(min_pts)\n",
    "            db = DBSCAN(eps=eps, min_samples=min_pts).fit(h)\n",
    "            c = db.labels_\n",
    "            e1 = self.get_effs(c, h, particle_id)\n",
    "            e2 = self.get_effs2(c, h, particle_id)\n",
    "            effs = {**e1, **e2}\n",
    "            for k, v in effs.items():\n",
    "                self.metrics[k].append(v)\n",
    "            self.clabels = db.labels_\n",
    "\n",
    "    def get_effs(self, c, h, particle_id):\n",
    "        c_id = pd.DataFrame({\"c\": c, \"id\": particle_id})\n",
    "        clusters = c_id.groupby(\"c\")\n",
    "        majority_pid = clusters[\"id\"].apply(lambda x: x.mode()[0])\n",
    "        majority_counts = clusters[\"id\"].apply(lambda x: sum(x == x.mode()[0]))\n",
    "        majority_fraction = clusters[\"id\"].apply(\n",
    "            lambda x: sum(x == x.mode()[0]) / len(x)\n",
    "        )\n",
    "        h_id = pd.DataFrame({\"hits\": np.ones(len(h)), \"id\": particle_id})\n",
    "        particles = h_id.groupby(\"id\")\n",
    "        nhits = particles[\"hits\"].apply(lambda x: len(x)).to_dict()\n",
    "        majority_hits = clusters[\"id\"].apply(lambda x: x.mode().map(nhits)[0])\n",
    "        perfect_match = (majority_hits == majority_counts) & (majority_fraction > 0.99)\n",
    "        double_majority = ((majority_counts / majority_hits).fillna(0) > 0.5) & (\n",
    "            majority_fraction > 0.5\n",
    "        )\n",
    "        lhc_match = (majority_fraction).fillna(0) > 0.75\n",
    "        return {\n",
    "            \"total\": len(c),\n",
    "            \"perfect\": sum(perfect_match),\n",
    "            \"double_majority\": sum(double_majority),\n",
    "            \"lhc\": sum(lhc_match),\n",
    "        }\n",
    "\n",
    "    def get_effs2(self, c, h, particle_id):\n",
    "        labels, labels_true = c, particle_id\n",
    "        return {\n",
    "            \"homogeneity\": metrics.homogeneity_score(labels_true, labels),\n",
    "            \"completeness\": metrics.completeness_score(labels_true, labels),\n",
    "            \"v_measure\": metrics.v_measure_score(labels_true, labels),\n",
    "            \"adjusted_rand_index\": metrics.adjusted_rand_score(labels_true, labels),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d4e69",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbscanner = DBScanner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ce3d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbscanner.scan(h, particle_id, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f8ba9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1a750",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dbscanner.metrics)\n",
    "df[\"dmn\"] = df[\"double_majority\"] / df[\"total\"]\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.pairplot(\n",
    "    df,\n",
    "    x_vars=[\"dmn\"],\n",
    "    y_vars=[\"homogeneity\", \"completeness\", \"v_measure\", \"adjusted_rand_index\"],\n",
    "    aspect=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf38bef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52626032",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from gnn_tracking.models.track_condensation_networks import PointCloudTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Utilizing {device}\")\n",
    "\n",
    "# use reference graph to get relevant dimensions\n",
    "p = pc_builder.data_list[0]\n",
    "node_indim = p.x.shape[1]\n",
    "hc_outdim = 2  # output dim of latent space\n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "point_clouds = pc_builder.data_list\n",
    "n_pcs = len(point_clouds)\n",
    "rand_array = uniform(low=0, high=1, size=n_pcs)\n",
    "train_pcs = [p for i, p in enumerate(point_clouds) if (rand_array <= 0.6)[i]]\n",
    "test_pcs = [\n",
    "    p\n",
    "    for i, p in enumerate(point_clouds)\n",
    "    if ((rand_array > 0.6) & (rand_array <= 0.8))[i]\n",
    "]\n",
    "val_pcs = [p for i, p in enumerate(point_clouds) if (rand_array > 0.8)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 2}\n",
    "train_loader = DataLoader(list(train_pcs), **params)\n",
    "params = {\"batch_size\": 1, \"shuffle\": False, \"num_workers\": 2}\n",
    "test_loader = DataLoader(list(test_pcs), **params)\n",
    "val_loader = DataLoader(list(val_pcs), **params)\n",
    "loaders = {\"train\": train_loader, \"test\": test_loader, \"val\": val_loader}\n",
    "print(\"Loader sizes:\", [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 0.1\n",
    "loss_functions = {\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    # \"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"potential_attractive\": 1,\n",
    "    \"potential_repulsive\": 10,\n",
    "    \"background\": 1 / 10,\n",
    "    # \"object\": 1/2500,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = PointCloudTCN(\n",
    "    node_indim, h_dim=8, e_dim=8, h_outdim=3, L=3, N_blocks=4, hidden_dim=100\n",
    ")\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"number trainable params:\", n_params)\n",
    "trainer = TCNTrainer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    loss_functions=loss_functions,\n",
    "    lr=0.001,\n",
    "    loss_weights=loss_weights,\n",
    "    device=device,\n",
    "    lr_scheduler=partial(StepLR, gamma=0.9, step_size=5),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de641",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0cfdb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0198962",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
